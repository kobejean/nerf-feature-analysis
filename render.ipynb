{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ccl/anaconda3/envs/NeRF/lib/python3.10/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import os\n",
    "import sys\n",
    "import importlib.util\n",
    "import tqdm\n",
    "\n",
    "from torch import Tensor\n",
    "import nerfacc\n",
    "import imageio\n",
    "import numpy as np\n",
    "\n",
    "from nerfacc.estimators.prop_net import (\n",
    "    PropNetEstimator,\n",
    "    get_proposal_requires_grad_fn,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp_dir = \"output/exp2\"\n",
    "root_fp = \"/home/ccl/Datasets/NeRF/aizu-student-hall/output/processed\"\n",
    "test_chunk_size=8192\n",
    "\n",
    "# Create a module spec\n",
    "spec = importlib.util.spec_from_file_location('ngp_appearance', f'{exp_dir}/ngp_appearance.py')\n",
    "ngp_appearance = importlib.util.module_from_spec(spec)\n",
    "spec.loader.exec_module(ngp_appearance)\n",
    "NGPDensityField = ngp_appearance.NGPDensityField\n",
    "NGPRadianceField = ngp_appearance.NGPRadianceField\n",
    "\n",
    "# spec = importlib.util.spec_from_file_location('nerf_colmap', f'{exp_dir}/nerf_colmap.py')\n",
    "# nerf_colmap = importlib.util.module_from_spec(spec)\n",
    "# spec.loader.exec_module(nerf_colmap)\n",
    "# SubjectLoader = nerf_colmap.SubjectLoader\n",
    "from datasets.nerf_colmap import SubjectLoader\n",
    "\n",
    "device = \"cuda:0\"\n",
    "# scene parameters\n",
    "unbounded = True\n",
    "aabb = torch.tensor([-1.0, -1.0, -1.0, 1.0, 1.0, 1.0], device=device)\n",
    "near_plane = 0.08  # TODO: Try 0.02\n",
    "far_plane = 1e3\n",
    "# dataset parameters\n",
    "train_dataset_kwargs = {\"color_bkgd_aug\": \"random\", \"factor\": 2}\n",
    "test_dataset_kwargs = {\"factor\": 4}\n",
    "# model parameters\n",
    "proposal_networks = [\n",
    "    NGPDensityField(\n",
    "        aabb=aabb,\n",
    "        unbounded=unbounded,\n",
    "        n_levels=5,\n",
    "        max_resolution=128,\n",
    "    ).to(device),\n",
    "    NGPDensityField(\n",
    "        aabb=aabb,\n",
    "        unbounded=unbounded,\n",
    "        n_levels=5,\n",
    "        max_resolution=256,\n",
    "    ).to(device),\n",
    "]\n",
    "\n",
    "estimator = PropNetEstimator().to(device)\n",
    "# radiance_field = NGPRadianceField(aabb=aabb, unbounded=unbounded, max_resolution=4096*2, n_levels=16, log2_hashmap_size=17).to(device)\n",
    "# radiance_field = NGPRadianceField(aabb=aabb, unbounded=unbounded, max_resolution=4096*4, n_levels=18, log2_hashmap_size=19).to(device)\n",
    "# radiance_field = NGPRadianceField(aabb=aabb, unbounded=unbounded, max_resolution=4096*8, n_levels=19, log2_hashmap_size=20).to(device)\n",
    "radiance_field = NGPRadianceField(aabb=aabb, unbounded=unbounded, max_resolution=4096*16, n_levels=20, log2_hashmap_size=21).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "odict_keys(['aabb', 'mlp_base.params'])\n",
      "odict_keys(['aabb', 'mlp_base.params'])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "PropNetEstimator()"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "radiance_field.load_state_dict(torch.load(os.path.join(exp_dir, 'radiance_field.pth')))\n",
    "# estimator.load_state_dict(torch.load(os.path.join(exp_dir, 'estimator.pth')))\n",
    "\n",
    "for i, net in enumerate(proposal_networks):\n",
    "    state_dict = torch.load(os.path.join(exp_dir, f'proposal_network_{i}.pth'))\n",
    "    print(state_dict.keys())\n",
    "    net.load_state_dict(state_dict)\n",
    "\n",
    "\n",
    "radiance_field.eval()\n",
    "for p in proposal_networks:\n",
    "    p.eval()\n",
    "estimator.eval()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "loading images\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 616/616 [00:01<00:00, 373.90it/s]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from datasets.utils import Rays\n",
    "from utils import (\n",
    "    render_image_with_propnet,\n",
    ")\n",
    "from datasets.nerf_colmap import _load_colmap, similarity_from_cameras\n",
    "\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "# render parameters\n",
    "num_samples = 128\n",
    "num_samples_per_prop = [512, 128]\n",
    "sampling_type = \"lindisp\"\n",
    "opaque_bkgd = True\n",
    "factor = 4\n",
    "\n",
    "images, camtoworlds, K, split_indices = _load_colmap(\n",
    "    root_fp, 0, factor\n",
    ")\n",
    "# normalize the scene\n",
    "T, sscale = similarity_from_cameras(\n",
    "    camtoworlds, strict_scaling=True\n",
    ")\n",
    "camtoworlds = np.einsum(\"nij, ki -> nkj\", camtoworlds, T)\n",
    "camtoworlds[:, :3, 3] *= sscale\n",
    "\n",
    "images = torch.from_numpy(images).to(torch.uint8).to(device)\n",
    "camtoworlds = (\n",
    "    torch.from_numpy(camtoworlds).to(torch.float32).to(device)\n",
    ")\n",
    "K = torch.tensor(K).to(torch.float32).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 616/616 [17:44<00:00,  1.73s/it]\n"
     ]
    }
   ],
   "source": [
    "def generate_rays(images, c2w, K):\n",
    "    height, width = images.shape[1:3]\n",
    "\n",
    "    x, y = torch.meshgrid(\n",
    "        torch.arange(width, device=images.device),\n",
    "        torch.arange(height, device=images.device),\n",
    "        indexing=\"xy\",\n",
    "    )\n",
    "    x = x.flatten()\n",
    "    y = y.flatten()\n",
    "\n",
    "    camera_dirs = F.pad(\n",
    "        torch.stack(\n",
    "            [\n",
    "                (x - K[0, 2] + 0.5) / K[0, 0],\n",
    "                (y - K[1, 2] + 0.5)\n",
    "                / K[1, 1]\n",
    "                * (1.0),\n",
    "            ],\n",
    "            dim=-1,\n",
    "        ),\n",
    "        (0, 1),\n",
    "        value=(1.0),\n",
    "    ) \n",
    "    directions = (camera_dirs[:, None, :] * c2w[:, :3, :3]).sum(dim=-1)\n",
    "    origins = torch.broadcast_to(c2w[:, :3, -1], directions.shape)\n",
    "    viewdirs = directions / torch.linalg.norm(directions, dim=-1, keepdims=True)\n",
    "    origins = torch.reshape(origins, (height, width, 3))\n",
    "    viewdirs = torch.reshape(viewdirs, (height, width, 3))\n",
    "\n",
    "    height, width = images.shape[1:3]\n",
    "\n",
    "    return Rays(origins=origins, viewdirs=viewdirs)\n",
    "\n",
    "\n",
    "def render_and_save_image(rays, images, index, radiance_field, proposal_networks, estimator, \n",
    "                          num_samples, num_samples_per_prop, near_plane, far_plane, \n",
    "                          sampling_type, opaque_bkgd, test_chunk_size, exp_dir):\n",
    "    color_bkgd = torch.ones(3, device=images.device)\n",
    "    img = torch.transpose(images[index], 0, 2)\n",
    "    img = torch.transpose(img, 1, 2)\n",
    "    img = (img / 255.0).unsqueeze(0).cuda()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        rgb, acc, depth, _ = render_image_with_propnet(\n",
    "            radiance_field,\n",
    "            proposal_networks,\n",
    "            estimator,\n",
    "            rays,\n",
    "            num_samples=num_samples,\n",
    "            num_samples_per_prop=num_samples_per_prop,\n",
    "            near_plane=near_plane,\n",
    "            far_plane=far_plane,\n",
    "            sampling_type=sampling_type,\n",
    "            opaque_bkgd=opaque_bkgd,\n",
    "            render_bkgd=color_bkgd,\n",
    "            test_chunk_size=test_chunk_size,\n",
    "            img=img,\n",
    "        )\n",
    "        renders_dir = os.path.join(exp_dir, \"renders\")\n",
    "        os.makedirs(renders_dir, exist_ok=True)\n",
    "\n",
    "        imageio.imwrite(\n",
    "            os.path.join(renders_dir, f\"rgb_{index:08}_render.png\"),\n",
    "            (rgb.cpu().numpy() * 255).astype(np.uint8),\n",
    "        )\n",
    "        vis_depth = torch.log(depth)\n",
    "        vis_depth -= torch.min(vis_depth)\n",
    "        vis_depth /= torch.max(vis_depth)\n",
    "        imageio.imwrite(\n",
    "            os.path.join(renders_dir, f\"rgb_{index:08}_depth.png\"),\n",
    "            (vis_depth.cpu().numpy() * 255).astype(np.uint8),\n",
    "        )\n",
    "\n",
    "for index in tqdm.tqdm(range(images.shape[0])):\n",
    "    image_id = [index]\n",
    "    c2w = camtoworlds[image_id]  # (1, 4, 4)\n",
    "    rays = generate_rays(images, c2w, K)\n",
    "    render_and_save_image(rays, images, index, radiance_field, proposal_networks, estimator, \n",
    "                        num_samples, num_samples_per_prop, near_plane, far_plane, \n",
    "                        sampling_type, opaque_bkgd, test_chunk_size, exp_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "NeRF",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
