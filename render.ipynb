{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ccl/anaconda3/envs/NeRF/lib/python3.10/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import os\n",
    "import sys\n",
    "import importlib.util\n",
    "import tqdm\n",
    "\n",
    "from torch import Tensor\n",
    "import nerfacc\n",
    "import imageio\n",
    "import numpy as np\n",
    "try:\n",
    "    import pytorch3d\n",
    "    from pytorch3d.transforms import matrix_to_quaternion, quaternion_to_matrix\n",
    "except ModuleNotFoundError:\n",
    "    pyt_version_str=torch.__version__.split(\"+\")[0].replace(\".\", \"\")\n",
    "    version_str=\"\".join([\n",
    "        f\"py3{sys.version_info.minor}_cu\",\n",
    "        torch.version.cuda.replace(\".\",\"\"),\n",
    "        f\"_pyt{pyt_version_str}\"\n",
    "    ])\n",
    "    !pip install fvcore iopath\n",
    "    !pip install --no-index --no-cache-dir pytorch3d -f https://dl.fbaipublicfiles.com/pytorch3d/packaging/wheels/{version_str}/download.html\n",
    "    \n",
    "\n",
    "from nerfacc.estimators.prop_net import (\n",
    "    PropNetEstimator,\n",
    "    get_proposal_requires_grad_fn,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp_dir = \"output/exp3\"\n",
    "root_fp = \"/home/ccl/Datasets/NeRF/aizu-student-hall/output/processed\"\n",
    "test_chunk_size=8192\n",
    "\n",
    "# Create a module spec\n",
    "spec = importlib.util.spec_from_file_location('ngp_appearance', f'{exp_dir}/ngp_appearance.py')\n",
    "ngp_appearance = importlib.util.module_from_spec(spec)\n",
    "spec.loader.exec_module(ngp_appearance)\n",
    "NGPDensityField = ngp_appearance.NGPDensityField\n",
    "NGPRadianceField = ngp_appearance.NGPRadianceField\n",
    "\n",
    "# spec = importlib.util.spec_from_file_location('nerf_colmap', f'{exp_dir}/nerf_colmap.py')\n",
    "# nerf_colmap = importlib.util.module_from_spec(spec)\n",
    "# spec.loader.exec_module(nerf_colmap)\n",
    "# SubjectLoader = nerf_colmap.SubjectLoader\n",
    "from datasets.nerf_colmap import SubjectLoader\n",
    "\n",
    "device = \"cuda:0\"\n",
    "# scene parameters\n",
    "unbounded = True\n",
    "aabb = torch.tensor([-1.0, -1.0, -1.0, 1.0, 1.0, 1.0], device=device)\n",
    "near_plane = 0.08  # TODO: Try 0.02\n",
    "far_plane = 1e3\n",
    "# dataset parameters\n",
    "train_dataset_kwargs = {\"color_bkgd_aug\": \"random\", \"factor\": 2}\n",
    "test_dataset_kwargs = {\"factor\": 4}\n",
    "# model parameters\n",
    "proposal_networks = [\n",
    "    NGPDensityField(\n",
    "        aabb=aabb,\n",
    "        unbounded=unbounded,\n",
    "        n_levels=5,\n",
    "        max_resolution=128,\n",
    "    ).to(device),\n",
    "    NGPDensityField(\n",
    "        aabb=aabb,\n",
    "        unbounded=unbounded,\n",
    "        n_levels=5,\n",
    "        max_resolution=256,\n",
    "    ).to(device),\n",
    "]\n",
    "\n",
    "estimator = PropNetEstimator().to(device)\n",
    "# radiance_field = NGPRadianceField(aabb=aabb, unbounded=unbounded, max_resolution=4096*2, n_levels=16, log2_hashmap_size=17).to(device)\n",
    "# radiance_field = NGPRadianceField(aabb=aabb, unbounded=unbounded, max_resolution=4096*4, n_levels=18, log2_hashmap_size=19).to(device)\n",
    "# radiance_field = NGPRadianceField(aabb=aabb, unbounded=unbounded, max_resolution=4096*8, n_levels=19, log2_hashmap_size=20).to(device)\n",
    "radiance_field = NGPRadianceField(aabb=aabb, unbounded=unbounded, max_resolution=4096*16, n_levels=20, log2_hashmap_size=21).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "odict_keys(['aabb', 'mlp_base.params'])\n",
      "odict_keys(['aabb', 'mlp_base.params'])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "PropNetEstimator()"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "radiance_field.load_state_dict(torch.load(os.path.join(exp_dir, 'radiance_field.pth')))\n",
    "# estimator.load_state_dict(torch.load(os.path.join(exp_dir, 'estimator.pth')))\n",
    "\n",
    "for i, net in enumerate(proposal_networks):\n",
    "    state_dict = torch.load(os.path.join(exp_dir, f'proposal_network_{i}.pth'))\n",
    "    print(state_dict.keys())\n",
    "    net.load_state_dict(state_dict)\n",
    "\n",
    "\n",
    "radiance_field.eval()\n",
    "for p in proposal_networks:\n",
    "    p.eval()\n",
    "estimator.eval()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "loading images\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 616/616 [00:05<00:00, 103.86it/s]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from datasets.utils import Rays\n",
    "from utils import (\n",
    "    render_image_with_propnet,\n",
    ")\n",
    "from datasets.nerf_colmap import _load_colmap, similarity_from_cameras\n",
    "\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "# render parameters\n",
    "num_samples = 128\n",
    "num_samples_per_prop = [512, 256]\n",
    "sampling_type = \"lindisp\"\n",
    "opaque_bkgd = True\n",
    "factor = 2\n",
    "\n",
    "images, camtoworlds, K, split_indices = _load_colmap(\n",
    "    root_fp, 0, factor\n",
    ")\n",
    "# normalize the scene\n",
    "T, sscale = similarity_from_cameras(\n",
    "    camtoworlds, strict_scaling=True\n",
    ")\n",
    "camtoworlds = np.einsum(\"nij, ki -> nkj\", camtoworlds, T)\n",
    "camtoworlds[:, :3, 3] *= sscale\n",
    "\n",
    "images = torch.from_numpy(images).to(torch.uint8).to(device)\n",
    "camtoworlds = (\n",
    "    torch.from_numpy(camtoworlds).to(torch.float32).to(device)\n",
    ")\n",
    "K = torch.tensor(K).to(torch.float32).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_rays(images, c2w, K):\n",
    "    height, width = images.shape[1:3]\n",
    "\n",
    "    x, y = torch.meshgrid(\n",
    "        torch.arange(width, device=images.device),\n",
    "        torch.arange(height, device=images.device),\n",
    "        indexing=\"xy\",\n",
    "    )\n",
    "    x = x.flatten()\n",
    "    y = y.flatten()\n",
    "\n",
    "    camera_dirs = F.pad(\n",
    "        torch.stack(\n",
    "            [\n",
    "                (x - K[0, 2] + 0.5) / K[0, 0],\n",
    "                (y - K[1, 2] + 0.5)\n",
    "                / K[1, 1]\n",
    "                * (1.0),\n",
    "            ],\n",
    "            dim=-1,\n",
    "        ),\n",
    "        (0, 1),\n",
    "        value=(1.0),\n",
    "    ) \n",
    "    directions = (camera_dirs[:, None, :] * c2w[:, :3, :3]).sum(dim=-1)\n",
    "    origins = torch.broadcast_to(c2w[:, :3, -1], directions.shape)\n",
    "    viewdirs = directions / torch.linalg.norm(directions, dim=-1, keepdims=True)\n",
    "    origins = torch.reshape(origins, (height, width, 3))\n",
    "    viewdirs = torch.reshape(viewdirs, (height, width, 3))\n",
    "\n",
    "    height, width = images.shape[1:3]\n",
    "\n",
    "    return Rays(origins=origins, viewdirs=viewdirs)\n",
    "\n",
    "\n",
    "def render_and_save_image(rays, images, index, a_vec, radiance_field, proposal_networks, estimator, \n",
    "                          num_samples, num_samples_per_prop, near_plane, far_plane, \n",
    "                          sampling_type, opaque_bkgd, test_chunk_size, exp_dir):\n",
    "    color_bkgd = torch.ones(3, device=images.device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        rgb, acc, depth, _ = render_image_with_propnet(\n",
    "            radiance_field,\n",
    "            proposal_networks,\n",
    "            estimator,\n",
    "            rays,\n",
    "            num_samples=num_samples,\n",
    "            num_samples_per_prop=num_samples_per_prop,\n",
    "            near_plane=near_plane,\n",
    "            far_plane=far_plane,\n",
    "            sampling_type=sampling_type,\n",
    "            opaque_bkgd=opaque_bkgd,\n",
    "            render_bkgd=color_bkgd,\n",
    "            test_chunk_size=test_chunk_size,\n",
    "            img=None,\n",
    "            a_vec=a_vec,\n",
    "        )\n",
    "        renders_dir = os.path.join(exp_dir, \"renders\")\n",
    "        os.makedirs(renders_dir, exist_ok=True)\n",
    "\n",
    "        imageio.imwrite(\n",
    "            os.path.join(renders_dir, f\"rgb_render_{index:08}.png\"),\n",
    "            (rgb.cpu().numpy() * 255).astype(np.uint8),\n",
    "        )\n",
    "        vis_depth = torch.log(depth)\n",
    "        vis_depth -= torch.min(vis_depth)\n",
    "        vis_depth /= torch.max(vis_depth)\n",
    "        imageio.imwrite(\n",
    "            os.path.join(renders_dir, f\"rgb_depth_{index:08}.png\"),\n",
    "            (vis_depth.cpu().numpy() * 255).astype(np.uint8),\n",
    "        )\n",
    "\n",
    "# for index in tqdm.tqdm(range(images.shape[0])):\n",
    "#     image_id = [index]\n",
    "#     c2w = camtoworlds[image_id]  # (1, 4, 4)\n",
    "#     rays = generate_rays(images, c2w, K)\n",
    "#     img = images[index]\n",
    "#     render_and_save_image(rays, images, index, img, radiance_field, proposal_networks, estimator, \n",
    "#                         num_samples, num_samples_per_prop, near_plane, far_plane, \n",
    "#                         sampling_type, opaque_bkgd, test_chunk_size, exp_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# image_ids = [413, 581, 70, 1]  # (N)\n",
    "\n",
    "# fig, axs = plt.subplots(1, len(image_ids), figsize=(15,15))\n",
    "\n",
    "# for i, image_id in enumerate(image_ids):\n",
    "#     axs[i].imshow(images[image_id].cpu())\n",
    "#     axs[i].set_title(f\"Image {image_id}\")\n",
    "#     axs[i].axis('off')\n",
    "\n",
    "# plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import matplotlib.pyplot as plt\n",
    "# W = 10\n",
    "# H = 10\n",
    "# fig, axs = plt.subplots(H,W, figsize=(15,15))\n",
    "\n",
    "# base_id=450\n",
    "# for i in range(H):\n",
    "#     for j in range(W):\n",
    "#         image_id = base_id + i*H+j\n",
    "#         axs[i,j].imshow(images[image_id].cpu())\n",
    "#         axs[i,j].set_title(f\"Image {image_id}\")\n",
    "#         axs[i,j].axis('off')\n",
    "\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# color_bkgd = torch.ones(3, device=images.device)\n",
    "# c2w = camtoworlds[[37]] \n",
    "# image_ids = [453, 457, 593, 199, 95, 154, 37, 457, 385, 335]\n",
    "# img = images[image_ids]\n",
    "# img = torch.permute(img, (0,3,1,2))\n",
    "# img = (img / 255.0).cuda()\n",
    "\n",
    "\n",
    "# fig, axs = plt.subplots(2, len(image_ids), figsize=(15,4))\n",
    "\n",
    "# with torch.no_grad():\n",
    "#     rays = generate_rays(images, c2w, K)\n",
    "#     print(rays.origins.shape)\n",
    "#     a_vec = radiance_field.appearance_encoding(img)  # (N, 48)\n",
    "\n",
    "# for index, image_id in enumerate(image_ids):\n",
    "#     with torch.no_grad():\n",
    "#         rgb, acc, depth, _ = render_image_with_propnet(\n",
    "#             radiance_field,\n",
    "#             proposal_networks,\n",
    "#             estimator,\n",
    "#             rays,\n",
    "#             num_samples=num_samples,\n",
    "#             num_samples_per_prop=num_samples_per_prop,\n",
    "#             near_plane=near_plane,\n",
    "#             far_plane=far_plane,\n",
    "#             sampling_type=sampling_type,\n",
    "#             opaque_bkgd=opaque_bkgd,\n",
    "#             render_bkgd=color_bkgd,\n",
    "#             test_chunk_size=test_chunk_size,\n",
    "#             img=None,\n",
    "#             a_vec=a_vec[index],\n",
    "#         )\n",
    "#     axs[0,index].imshow(rgb.cpu())\n",
    "#     axs[0,index].set_title(f\"Render {image_id}\")\n",
    "#     axs[0,index].axis('off')\n",
    "#     axs[1,index].imshow(img[[index]].permute((0,2,3,1)).squeeze().cpu())\n",
    "#     axs[1,index].set_title(f\"Image {image_id}\")\n",
    "#     axs[1,index].axis('off')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def interpolate_appearance(a_vec, M):\n",
    "    # Determine the number of segments and frames per segment\n",
    "    segments = a_vec.shape[0] - 1\n",
    "    frames_per_segment = M // segments\n",
    "\n",
    "    new_a_vecs = []\n",
    "\n",
    "    for i in range(segments):\n",
    "        # Prepare the target number of frames for this segment\n",
    "        if i == segments - 1:  # last segment - can be slightly longer\n",
    "            frames = frames_per_segment + M % segments\n",
    "        else:\n",
    "            frames = frames_per_segment\n",
    "\n",
    "        output_frames = torch.linspace(0, 1, frames, device=a_vec.device)\n",
    "\n",
    "        # Interpolate this segment\n",
    "        for t in output_frames:\n",
    "            new_a_vecs.append(torch.lerp(a_vec[i], a_vec[i+1], t))\n",
    "\n",
    "    # Concatenate all segments\n",
    "    new_a_vecs = torch.stack(new_a_vecs)\n",
    "\n",
    "    return new_a_vecs\n",
    "\n",
    "def slerp(q1, q2, t):\n",
    "    \"\"\"Spherical linear interpolation between two quaternions.\"\"\"\n",
    "    dot = torch.dot(q1 / torch.norm(q1), q2 / torch.norm(q2))\n",
    "    dot = torch.clamp(dot, -1, 1)  # Avoid invalid values due to numerical errors\n",
    "    theta = torch.acos(dot) * t\n",
    "    q3 = (q2 - q1 * dot)\n",
    "    q3 = q3 / torch.norm(q3)\n",
    "    return torch.cos(theta) * q1 + torch.sin(theta) * q3\n",
    "\n",
    "\n",
    "\n",
    "def interpolate_transforms(c2w, M):\n",
    "    # Separate rotation and translation\n",
    "    rotations = c2w[:, :3, :3]  # shape: (N, 3, 3)\n",
    "    translations = c2w[:, :3, 3]  # shape: (N, 3)\n",
    "\n",
    "    # Convert rotations to quaternions for smooth interpolation\n",
    "    quaternions = matrix_to_quaternion(rotations)\n",
    "\n",
    "    segments = len(quaternions) - 1\n",
    "    frames_per_segment = M // segments\n",
    "\n",
    "    new_quaternions = []\n",
    "    new_translations = []\n",
    "\n",
    "    for i in range(segments):\n",
    "        # Prepare the target number of frames for this segment\n",
    "        if i == segments - 1:  # last segment - can be slightly longer\n",
    "            frames = frames_per_segment + M % segments\n",
    "        else:\n",
    "            frames = frames_per_segment\n",
    "\n",
    "        output_frames = torch.linspace(0, 1, frames, device=c2w.device)\n",
    "\n",
    "        # Interpolate this segment\n",
    "        for t in output_frames:\n",
    "            new_quaternions.append(slerp(quaternions[i], quaternions[i+1], t))\n",
    "            new_translations.append(torch.lerp(translations[i], translations[i+1], t))\n",
    "\n",
    "\n",
    "    # Concatenate all segments\n",
    "    new_quaternions = torch.cat(new_quaternions).view(M, -1)\n",
    "    new_translations = torch.cat(new_translations).view(M, -1)\n",
    "    # Convert quaternions back to rotation matrices\n",
    "    new_rotations = quaternion_to_matrix(new_quaternions)  # shape: (M, 3, 3)\n",
    "\n",
    "    # Combine new rotations and translations\n",
    "    new_c2w = torch.zeros((M, 4, 4))\n",
    "    new_c2w[:, :3, :3] = new_rotations\n",
    "    new_c2w[:, :3, 3] = new_translations\n",
    "    new_c2w[:, 3, 3] = 1\n",
    "\n",
    "    return new_c2w\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [2:43:53<00:00,  9.83s/it] \n"
     ]
    }
   ],
   "source": [
    "\n",
    "M = 1000\n",
    "\n",
    "# image_ids = [413, 581, 70, 1]# (N)\n",
    "# image_ids = [37, 95, 70, 1]# (N)\n",
    "image_ids = [453, 457, 593, 199, 95, 154, 37, 457, 385, 335] * 2 + [453]# (N)\n",
    "c2w_ids = [1, 513, 95, 51, 596, 488, 483, 522, 546, 1] # (N)\n",
    "c2w = camtoworlds[c2w_ids]  # (N, 4, 4)\n",
    "\n",
    "# M = len(c2w_ids)\n",
    "\n",
    "# manual modifications\n",
    "c2w[0,1,3] += 0.03\n",
    "c2w[0,0,3] += 0.04\n",
    "c2w[-1,1,3] += 0.03\n",
    "c2w[-1,0,3] += 0.04\n",
    "\n",
    "img = images[image_ids]\n",
    "img = torch.permute(img, (0,3,1,2))\n",
    "img = (img / 255.0).cuda()\n",
    "\n",
    "with torch.no_grad():\n",
    "    a_vec = radiance_field.appearance_encoding(img)  # (N, 48)\n",
    "\n",
    "new_c2w = interpolate_transforms(c2w, M).cuda()\n",
    "new_a_vec = interpolate_appearance(a_vec, M)  # (M, 48)\n",
    "\n",
    "for index in tqdm.tqdm(range(M)):\n",
    "    rays = generate_rays(images, new_c2w[[index]], K)\n",
    "    render_and_save_image(rays, images, index, new_a_vec[[index]], radiance_field, proposal_networks, estimator, \n",
    "                        num_samples, num_samples_per_prop, near_plane, far_plane, \n",
    "                        sampling_type, opaque_bkgd, test_chunk_size, exp_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "NeRF",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
